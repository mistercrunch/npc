---
title: "HISTORY"
ezine: "0blivion"
---

# HISTORY

**Ezine:** 0blivion

<div className="ascii-content">

```
           _____________________________________________________
          /              Oblivion Underground Magazine          \
         /                   Issue 2    15/04/2000               \
        ▌            History, Present and Future of TCP/IP        ▌
         \                         by Slider                     /
          \_____________________________________________________/

- SOF -

Introduction to TCP/IP - History, Present and the Future.

Today, the Internet, World Wide Web, and Information Super Highway are
familiar terms to millions of people all over the world. Transmission Control
Protocol/Internet Protocol (TCP/IP) is the protocol suite developed for the
Internet.

In this file I describe how the Internet was formed, how it developed and how
it is likely to develop in the future.

- Internet History - Where It All Came From

Networks have become a fundamental, if not the most important, part of today's
information systems. They form the backbone for information sharing in
enterprises, governmental and scientific groups. That information can take
several forms. It can be notes and documents, data to be processed by another
computer, files sent to colleagues, and even more exotic forms of data.

Most of these networks were installed in the late 60s and 70s, when network
design was the "state of the art" topic of computer research and sophisticated
implementers. It resulted in multiple networking models such as
packet-switching technology, collision-detection local area networks,
hierarchical enterprise networks, and many other excellent technologies.

From the early 70s on, another aspect of networking became important: protocol
layering, which allows applications to communicate with each other. A complete
range of architectural models were proposed and implemented by various
research teams and computer manufacturers.

The result of all this great know-how is that today any group of users can
find a physical network and an architectural model suitable for their specific
needs. This ranges from cheap asynchronous lines with no other error recovery
than a bit-per-bit parity function, through full-function wide area networks
(public or private) with reliable protocols such as public packet-switching
networks or private SNA networks, to high-speed but limited-distance local
area networks.

The down side of this exploding information sharing is the rather painful
situation when one group of users wants to extend its information system to
another group of users who happen to have a different network technology and
different network protocols. As a result, even if they could agree on a type
of network technology to physically interconnect the two locations, their
applications (such as mailing systems) still would not be able to communicate
with each other because of the different protocols.

This situation was recognized rather early (beginning of the 70s) by a group
of researchers in the U.S. who came up with a new principle: internetworking.
Other official organizations became involved in this area of interconnecting
networks, such as ITU-T (formerly CCITT) and ISO. All were trying to define a
set of protocols, layered in a well-defined suite, so that applications would
be able to talk to other applications, regardless of the underlying network
technology and the operating systems where those applications run.

- Internetworks

Those original designers, funded by the  Defense Advanced Research Projects
Agency (DARPA), of the  ARPANET protocol suite introduced fundamental concepts
such as  layering and  virtualizing in the world of networking, well before
ISO even took an interest in networking.

The official organization of those researchers was the ARPANET Network Working
Group, which had its last general meeting in October 1971. DARPA continued its
research for an internetworking protocol suite, from the early  NCP (Network
Control Program) host-to-host protocol to the TCP/IP protocol suite, which
took its current form around 1978. At that time, DARPA was well known for its
pioneering of packet-switching over radio networks and satellite channels. The
first real implementations of the  Internet were found around 1980 when DARPA
started converting the machines of its research network (ARPANET) to use the
new TCP/IP protocols. In 1983, the transition was completed and DARPA demanded
that  all computers willing to connect to its ARPANET use TCP/IP.

DARPA also contracted Bolt, Beranek, and Newman (BBN) to develop an
implementation of the TCP/IP protocols for Berkeley UNIX on the VAX and funded
the University of California at Berkeley to distribute that code free of
charge with their UNIX operating system. The first release of the Berkeley
Software Distribution to include the TCP/IP protocol set was made available in
1983 (4.2BSD). From that point on, TCP/IP spread rapidly among universities
and research centers and has become the standard communications subsystem for
all UNIX connectivity.

The second release (4.3BSD) was distributed in 1986, with updates in 1988
(4.3BSD Tahoe) and 1990 (4.3BSD Reno). 4.4BSD was released in 1993. Due to
funding constraints, 4.4BSD was the last release of the BSD by the Computer
Systems Research Group of the University of California at Berkeley.

As TCP/IP internetworking spread rapidly, new wide area networks were created
in the U.S. and connected to ARPANET. In turn, other networks in the rest of
the world, not necessarily based on the TCP/IP protocols, were added to the
set of interconnected networks. The result is what is described as  The
Internet. Some examples of the different networks that have played key roles
in this development are described in the next sections.

- The Internet

What exactly is the Internet? First, the word internet (also internetwork) is
simply a contraction of the phrase  interconnected network. However, when
written with a capital *I* the Internet refers to a worldwide set of
interconnected networks, so the Internet is an internet, but the reverse does
not apply. The Internet is sometimes called the  connected Internet.

The Internet consists of the following groups of networks (see the following
sections for more information on some of these):

* Backbones: large networks that exist primarily to interconnect other
  networks.  Currently the backbones are NSFNET in the US, EBONE in Europe,
  and large commercial backbones.
* Regional networks connecting, for example, universities and colleges.
* Commercial networks providing access to the backbones to subscribers, and
  networks owned by commercial organizations for internal use that also have
  connections to the Internet.
* Local networks, such as campus-wide university networks.

In many cases, particularly for commercial, military and government networks,
traffic between these networks and the rest of the Internet is restricted.

- ARPANET

Sometimes referred to as the *grand-daddy of packet networks*, the ARPANET was
built by DARPA (which was called ARPA at that time) in the late 60s to
accommodate research equipment on packet-switching technology and to allow
resource sharing for the Department of Defense's contractors. The network
interconnected research centers, some military bases and government locations.
It soon became popular with researchers for collaboration through electronic
mail and other services. It was developed into a research utility run by the
Defense Communications Agency (DCA) by the end of 1975 and split in 1983 into
MILNET for interconnection of military sites and ARPANET for interconnection
of research sites. This formed the beginning of the *capital I* Internet.

In 1974, the ARPANET was based on 56 Kbps leased lines that interconnected
packet-switching nodes (PSN) scattered across the continental U.S. and western
Europe. These were minicomputers running a protocol known as  1822 (after the
number of a report describing it) and dedicated to the packet-switching task.
Each PSN had at least two connections to other PSNs (to allow alternate
routing in case of circuit failure) and up to 22 ports for user computer
(host) connections. These 1822 systems offered reliable, flow-controlled
delivery of a packet to a destination node. This is the reason why the
original  NCP protocol was a rather simple protocol. It was replaced by the
TCP/IP protocols, which do not assume reliability of the underlying network
hardware and can be used on other-than-1822 networks.

This 1822 protocol did not become an industry standard, so DARPA decided later
to replace the 1822 packet switching technology with the  CCITT X.25 standard.
Data traffic rapidly exceeded the capacity of the 56 Kbps lines that made up
the network, which were no longer able to support the necessary throughput.
Today the ARPANET has been replaced by new technologies in its role of
backbone on the research side of the connected Internet (see NSFNET later in
this chapter), whereas MILNET continues to form the backbone of the military
side.

- NSFNET

NSFNET, the National Science Foundation Network, is a three-level internetwork
in the United States consisting of:

* The  backbone: a network that connects separately administered and operated
  mid-level networks and NSF-funded supercomputer centers. The backbone
  also has transcontinental links to other networks such as EBONE, the
  European IP backbone network.
* Mid-level networks: of three kinds (regional, discipline-based and
  supercomputer consortium networks).
* Campus networks: whether academic or commercial, connected to the
  mid-level networks.

First Backbone

	Originally established by the National Science Foundation (NSF) as a
	communications network for researchers and scientists to access the
	NSF supercomputers, the first NSFNET backbone used six DEC
	LSI/11 microcomputers as packet switches, interconnected by 56
	Kbps leased lines. A primary interconnection between the NSFNET
	backbone and the ARPANET existed at Carnegie Mellon, which
	allowed routing of datagrams between users connected to each of
	those networks.

Second Backbone
	
	The need for a new backbone appeared in 1987, when the first one
	became overloaded within a few months (estimated growth at that
	time was 100% per year). The NSF and MERIT, Inc., a computer
	network consortium of eight state-supported universities in Michigan,
	agreed to develop and manage a new, higher-speed backbone with
	greater transmission and switching capacities. To manage it they
	defined the  Information Services (IS) which is comprised of an
	Information Center and a Technical Support Group. The Information
	Center is responsible for information dissemination, information
	resource management and electronic communication. The Technical
	Support Group provides support directly to the field. The purpose of
	this is to provide an integrated information system with
	easy-to-use-and-manage interfaces accessible from any point in the
	network supported by a full set of training services.

	Merit and NSF conducted this project in partnership with IBM and
	MCI. IBM provided the software, packet-switching and
	network-management equipment, while MCI provided the
	long-distance transport facilities. Installed in 1988, the new network
	initially used 448 Kbps leased circuits to interconnect 13 nodal
	switching systems (NSS) supplied by IBM. Each NSS was composed
	of nine IBM RISC systems (running an IBM version of 4.3BSD UNIX)
	loosely coupled via two IBM Token-Ring Networks (for redundancy).
	One Integrated Digital Network Exchange (IDNX) supplied by IBM was
	individual at each of the 13 locations, to provide:

	* Dynamic alternate routing
	* Dynamic bandwidth allocation

Third Backbone

	In 1989, the NSFNET backbone circuits topology was reconfigured
	after traffic measurements and the speed of the leased lines
	increased to T1 (1.544 Mbps) using primarily fiber optics.
	Due to the constantly increasing need for improved packet switching
	and transmission capacities, three NSSs were added to the backbone
	and the link speed was upgraded. The migration of the NSFNET
	backbone from T1 to T3 (45Mbps) was completed in late 1992. The
	subsequent migration to gigabit levels has already started and will
	continue through the late 1990s.

In April 1995 the US government discontinued its funding of NSFNET. This was
in part a reaction to growing commercial use of the network. About the same
time, NSFNET gradually migrated the main backbone traffic in the U.S. to
commercial network service providers, and NSFNET reverted to being a network
for the research community. The main backbone network is now run in
cooperation with MCI and is known as the vBNS (very high speed Backbone
Network Service).

NSFNET has played a key role in the development of the Internet. However, many
other networks have also played their part and/or also make up a part of the
Internet today.

- Commercial Use of the Internet

In recent years the Internet has grown in size and range at a greater rate
than anyone could have predicted. A number of key factors have influenced
this growth. Some of the most significant milestones have been the free
distribution of Gopher in 1991, the first posting, also in 1991, of the
specification for hypertext and, in 1993, the release of Mosaic, the first
graphics-based browser. Today the vast majority of the hosts now connected to
the Internet are of a commercial nature.

This is an area of potential and actual conflict with the initial aims of the
Internet, which were to foster open communications between academic and
research institutions. However, the continued growth in commercial use of the
Internet is inevitable so it will be helpful to explain how this evolution is
taking place.

One important initiative to consider is that of the Acceptable Use Policy
(AUP). The first of these policies was introduced in 1992 and applies to the
use of NSFNET. A copy of this can be obtained at

nic.merit.edu/nsfnet/acceptable.use.policy.

At the heart of this AUP is a commitment "to support open research and
education".

Under "Unacceptable Uses" is a prohibition of "use for for-profit activities",
unless covered by the General Principle or as a specifically acceptable use.
However, in spite of this apparently restrictive stance the NSFNET was
increasingly used for a broad range of activities, including many of a
commercial nature, before reverting to its original objectives in 1995.

The provision of an AUP is now commonplace among Internet Service Providers,
although the AUP has generally evolved to be more suitable for commercial use.
Some networks still provide services free of any AUP.

Let us now focus on the Internet service providers who have been most active
in introducing commercial uses to the Internet. Two worth mentioning are
PSINet and UUNET, which began in the late 80s to offer Internet access to both
businesses and individuals. The California-based CERFnet provided services
free of any AUP. An organization to interconnect PSINet, UUNET and CERFnet was
formed soon after, called the Commercial Internet Exchange (CIX), based on the
understanding that the traffic of any member of one network may flow without
restriction over the networks of the other members. As of July 1997, CIX had
grown to more than 146 members from all over the world connecting member
internets. At about the same time that CIX was formed, a non-profit company,
Advance Network and Services (ANS), was formed by IBM, MCI and Merit, Inc. to
operate T1 (subsequently T3) backbone connections for NSFNET. This group was
active in increasing the commercial presence on the Internet.

ANS formed a commercially oriented subsidiary called ANS CO+RE to provide
linkage between commercial customers and the research and education domains.
ANS CO+RE provides access to NSFNET as well as being linked to CIX. In 1995
ANS was acquired by America Online.

In 1995, as the NSFNET was reverting to its previous academic role, the
architecture of the Internet changed from having a single dominant backbone in
the U.S. to having a number of commercially operated backbones. In order for
the different backbones to be able to exchange data, the NSF set up four
Network

Access Points (NAPs) to serve as data interchange points between the backbone
service providers.

Another type of interchange is the Metropolitan Area Ethernet (MAE). Several
MAEs have been set up by Metropolian Fiber Systems (MFS), who also have their
own backbone network. NAPs and MAEs are also referred to as public exchange
points (IXPs). Internet Service Providers (ISPs) typically will have
connections to a number of IXPs for performance and backup.

Similar to CIX in the United States, European Internet providers formed the
RIPE (RΘseaux IP EuropΘens) organization to ensure technical and
administrative coordination. RIPE was formed in 1989 to provide a uniform IP
service to users throughout Europe. Currently, more than 1000 organizations
participate in RIPE, and close to 6 million hosts (as of February 1998) could
be reached via RIPE-coordinated networks.

Today, the largest Internet backbones run at OC3 (155 Mbps) or OC12
(622 Mbps).  By late 1998 OC12 should be the standard speed for major
backbones.

- Information Superhighway

One recent and important initiative was the creation of the U.S. Advisory
Council on the National Information Infrastructure (NIIAC) headed by U.S.
Vice President Al Gore (who has been credited with coining the phrase
*information superhighway*).

The Advisory Council, which was made up of representatives from many areas of
industry, government, entertainment and education, met for a period of two
years from 1994-6. At the end of their term, they concluded their work with
the publishing of two major reports:

* Kickstart Initiative: Connecting America's Communities to the Information
  Superhighway
* A Nation of Opportunity: Realizing the Promise of the Information
  Superhighway

Among the findings in these reports are the goal that every person in the U.S.
should have access to the Internet by the year 2005, with all schools and
libraries being connected by the year 2000.

Although the reports do not specify direct government funding for expansion of
the Internet, preferring "commercial and competitive initiatives" to be the
driving force, it does give a responsibility to all levels of government to
ensure fair access and remove regulatory obstacles.

The most recent and substantive government affirmation for the Internet came,
in 1996, in the form of the Next Generation Internet initiative. This was
launched by the Clinton administration with the goals of:

* Connecting universities and national labs with networks that are 100-1000
  times faster than today's (as of October 1996) Internet.
* Promote expermentation with the next generation of networking technologies.
* Demonstrate new applications that meet important national goals and
  missions.

The initiative included funding of $100 million for 1998.


- Internet2

The success of the Internet and the subsequent frequent congestion of the
NSFNET and its commercial replacement led to some frustration among the
research community who had previously enjoyed exclusive use of the Internet.
The university community, therefore, together with government and industry
partners, and encouraged by the funding component of the NGI, have formed the
Internet2 project.

Internet2 has the following principle objectives:

* To create a high bandwidth, leading-edge network capability for the research
  community in the U.S.

* To enable a new generation of applications and communication technologies to
  fully exploit the capabilities of broadband networks.

* To rapidly transfer newly developed technologies to all levels of education
  and to the broader Internet community, both in the U.S. and abroad.


- The Open Systems Interconnect (OSI) Model

Around the same time that DARPA was researching for an internetworking
protocol suite in response to the requirement for the establishment of
networking standards, which eventually led to TCP/IP and the Internet, an
alternative standards approach was being led by the CCITT (ComitΘ Consultatif
International Telephonique et Telegraphique, or Consultative Committee on
International Telephony and Telegraphy), and the ISO (International
Organization for Standardization ). The CCITT has since become the ITU-T
(International Telecommunications Union - Telecommunication Standardization
Sector).

This effort resulted in the OSI (Open Systems Interconnect) Reference Model
(ISO 7498), which defined a seven-layer model of data communication with
physical transport at the lower layer and application protocols at the upper
layers.

This model, is widely accepted as a basis for the understanding of how a
network protocol stack should operate and as a reference tool for comparing
network stack implementations.

The OSI Reference Model has seven layers; each layer provides a set of
functions to the layer above and, in turn, relies on the functions provided
by the layer below. Although messages can only pass vertically through the
stack from layer to layer, from a logical point of view, each layer
communicates directly with its peer layer on other nodes.

The seven layers are:

Application
Network applications such as terminal emulation and file transfer

Presentation
Formatting of data and encryption

Session
Establishment and maintenance of sessions

Transport
Provision of reliable and unreliable end-to-end delivery

Network
Packet delivery, including routing

Data Link
Framing of units of information and error checking

Physical
Transmission of bits on the physical hardware

The two standards processes approach standardization from two different
perspectives. The OSI approach started from a clean slate and defined
standards, adhering tightly to their own model, using a formal committee
process without requiring implementations. The Internet uses a less formal
engineering approach, where anybody can propose and comment on RFCs, and
implementations are required to verify feasibility. The OSI protocols
developed slowly, and because running the full protocol stack is resource
intensive, they have not been widely deployed, especially in the desktop and
small computer market. In the meantime, TCP/IP and the Internet were
developing rapidly and being put into use.

- X.500: The Directory Service Standard

The OSI protocols did, however, address issues important in large distributed
systems that were developing in an ad hoc manner in the desktop and Internet
marketplace. One such important area was directory services. The CCITT created
the X.500 standard in 1988, which became ISO 9594, Data Communications
Network Directory, Recommendations X.500-X.521 in 1990, though it is still
commonly referred to as X.500.

X.500 organizes directory entries in a hierarchical name space capable of
supporting large amounts of information. It also defines powerful search
capabilities to make retrieving information easier. Because of its
functionality and scalability, X.500 is often used together with add-on
modules for interoperation between incompatible directory services. X.500
specifies that communication between the directory client and the directory
server uses the Directory Access Protocol (DAP).

- The IP Address Exhaustion Problem

The number of networks on the Internet has been approximately doubling
annually for a number of years. However, the usage of the Class A, B and C
networks differs greatly. Nearly all of the new networks assigned in the late
1980s were Class B, and in 1990 it became apparent that if this trend
continued, the last Class B network number would be assigned during 1994.
On the other hand, Class C networks were hardly being used.

The reason for this trend was that most potential users found a Class B
network to be large enough for their anticipated needs, since it accommodates
up to 65534 hosts, whereas a class C network, with a maximum of 254 hosts,
severely restricts the potential growth of even a small initial network.
Furthermore, most of the class B networks being assigned were small ones.
There are relatively few networks that would need as many as 65,534 host
addresses, but very few for which 254 hosts would be an adequate limit. In
summary, although the Class A, Class B and Class C divisions of the IP address
are logical and easy-to-use (because they occur on byte boundaries), with
hindsight they are not the most practical because Class C networks are too
small to be useful for most organizations while Class B networks are too large
to be densely populated by any but the largest organizations.

In May 1996, there were all of Class A addresses either allocated or assigned,
as well as 61.95 percent of Class B and 36.44 percent of Class C IP network
addresses. The terms assigned and allocated in this context have the following
meanings:

Assigned

The number of network numbers in use. The Class C figures are somewhat
inaccurate, because the figures do not include many class C networks in
Europe, which were allocated to RIPE and subsequently assigned but which
are still recorded as allocated.

Allocated

This includes all of the assigned networks and additionally, those networks
that have either been reserved by IANA (for example, the 63 class A networks
are all reserved by IANA) or have been allocated to regional registries by
IANA and will subsequently be assigned by those registries.

Another way to look at these numbers is to examine the proportion of the
address space that has been used. The figures in the table do not show for
example that the Class A address space is as big as the rest combined, or that
a single Class A network can theoretically have as many hosts as 66,000 Class
C networks.

Since 1990, the number of assigned Class B networks has been increasing at a
much lower rate than the total number of assigned networks and the anticipated
exhaustion of the Class B network numbers has not yet occurred. The reason for
this is that the policies of the InterNIC on network number allocation were
changed in late 1990 to preserve the existing address space, in particular to
avert the exhaustion of the Class B address space. The new policies can be
summarized as follows.

- The upper half of the Class A address space (network numbers 64 to 127) is
  reserved indefinitely to allow for the possibility of using it for
  transition to a new numbering scheme.

- Class B networks are only assigned to organizations that can clearly
  demonstrate a need for them. The same is, of course, true for Class A
  networks. The requirements for Class B networks are that the requesting
  organization:

* Has a subnetting plan that documents more than 32 subnets within its
  organizational network

* Has more than 4096 hosts
  Any requirements for a Class A network would be handled on an individual
  case basis.

* Organizations that do not fulfill the requirements for a Class B network are
  assigned a consecutively numbered block of Class C network numbers.

* The lower half of the Class C address space (network numbers 192.0.0 through
  207.255.255) is divided into eight blocks, which are allocated to regional
  authorities as follows:

192.0.0 - 193.255.255 Multi-regional
194.0.0 - 195.255.255 Europe
196.0.0 - 197.255.255 Others
198.0.0 - 199.255.255 North America
200.0.0 - 201.255.255 Central and South America
202.0.0 - 203.255.255 Pacific Rim
204.0.0 - 205.255.255 Others
206.0.0 - 207.255.255 Others

The ranges defined as Others are to be where flexibility outside the
constraints of regional boundaries is required. The range defined as
multi-regional includes the Class C networks that were assigned before this
new scheme was adopted. The 192 networks were assigned by the InterNIC and
the 193 networks were previously allocated to RIPE in Europe.

The upper half of the Class C address space (208.0.0 to 223.255.255) remains
unassigned and unallocated.

* Where an organization has a range of class C network numbers, the range
  provided is assigned as a  bit-wise contiguous range of network numbers, and 
  the number of networks in the range is a power of 2. That is, all IP
  addresses in the range have a common prefix, and every address with that
  prefix is within the range. For example, a European organization requiring
  1500 IP addresses would be assigned eight Class C network numbers (2048
  IP addresses) from the number space reserved for European networks
  (194.0.0 through 195.255.255) and the first of these network numbers would be
  divisible by eight. A range of addresses satisfying these rules would be
  194.32.136 through 194.32.143, in which case the range would consist of all
  of the IP addresses with the 21-bit prefix 194.32.136, or
  B'110000100010000010001'.

The maximum number of network numbers assigned contiguously is 64,
corresponding to a prefix of 18 bits. An organization requiring more than 4096
addresses but less than 16,384 addresses can request either a Class B or a
range of Class C addresses. In general, the number of Class C networks
assigned is the minimum required to provide the necessary number of IP
addresses for the organization on the basis of a two-year outlook. However, in
some cases, an organization can request multiple networks to be treated
separately. For example, an organization with 600 hosts would normally be
assigned four class C networks. However, if those hosts were distributed
across 10 token-ring LANs with between 50 and 70 hosts per LAN, such an
allocation would cause serious problems, since the organization would have to
find 10 subnets within a 10-bit local address range. This would mean at least
some of the LANs having a subnet mask of 255.255.255.192 which allows only
62 hosts per LAN. The intent of the rules is not to force the organization
into complex subnetting of small networks, and the organization should request
10 different Class C numbers, one for each LAN.

The use of Class C network numbers in this way has averted the
exhaustion of the Class B address space, but it is not a permanent term
solution to the overall address space constraints that are fundamental to IP.
A long-term solution is needed and its called IP Version 6.

But, this will be included in a later text for Oblivion Mag.

Slider.

- EOF -

Shouts : Anyone that knows me.

```

</div>
